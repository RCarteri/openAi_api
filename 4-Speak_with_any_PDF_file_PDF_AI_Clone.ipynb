{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCarteri/openAi_api/blob/main/Speak_with_any_PDF_file_PDF_AI_Clone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUehOJlORE-j"
      },
      "source": [
        "# Project 4: Talk with any Document - Integrating ChatCompletion API, Embeddings, and Pinecone\n",
        "\n",
        "In this advanced section of our course, we're going to build a highly interactive and intelligent system that lets users 'talk' with any document. Leveraging the capabilities of OpenAI's ChatCompletion API, the semantic understanding of embeddings, we'll create an application that can understand and retrieve information from documents in a conversational manner.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "- **Integration of OpenAI Services**: Understand how to seamlessly integrate various OpenAI services such as ChatCompletion API and Embeddings to create a powerful AI system.\n",
        "- **Pinecone for Vector Searching**: Get acquainted with Pinecone, a vector database perfect for handling complex queries over embeddings, to efficiently index and retrieve document information.\n",
        "- **Natural Language Understanding**: Enhance the system's ability to comprehend and process human language within documents for more natural interactions.\n",
        "- **User Interface for Document Interaction**: Build a user-friendly interface that allows users to upload documents and engage in conversations with the content.\n",
        "- **Conversational Context Management**: Develop strategies to maintain the context of the conversation, ensuring relevant and accurate responses.\n",
        "\n",
        "## Project Objectives\n",
        "\n",
        "By the end of this project, you will have developed a system that can:\n",
        "\n",
        "1. **Interpret Documents**: Analyze and understand the content of various documents through the power of embeddings.\n",
        "2. **Conversational Interface**: Provide users with the ability to ask questions and receive answers as if they were talking to a human expert on the document's content.\n",
        "3. **Contextual Awareness**: Maintain the thread of conversation, taking into account previous interactions and the document's subject matter.\n",
        "4. **Scalable Document Handling**: Efficiently manage and query a large number of documents using Pinecone's vector database capabilities.\n",
        "\n",
        "## Preparation Checklist\n",
        "\n",
        "Before we dive in, make sure you have:\n",
        "\n",
        "- A Google Colab account.\n",
        "- A foundational understanding of Python, APIs, and natural language processing concepts.\n",
        "- An OpenAI API key with access to the ChatCompletion and Embeddings features ([OpenAI](https://platform.openai.com/account/api-keys)).\n",
        "- Familiarity with LangChain and Pinecone services.\n",
        "\n",
        "## Ready to Talk with Documents?\n",
        "\n",
        "We are about to transform how you interact with text-based information. Prepare to build a conversational bridge between users and the vast world of documents!\n",
        "\n",
        "NOTE:\n",
        "\n",
        "Retrieval-augmented generation (RAG) for large language models (LLMs) aims to improve prediction quality by using an external datastore at inference time to build a richer prompt that includes some combination of context, history, and recent/relevant knowledge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDlbXftjSHdL"
      },
      "source": [
        "# 2. Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTBw517ySLF2",
        "outputId": "f497eff2-dcc2-458e-f351-94f78facea8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.35.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: certifi in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_EmMwxN9LBg",
        "outputId": "81201aeb-8e51-4730-8f30-33a78c8f59d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
            "   ------------------- -------------------- 112.6/232.6 kB 2.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 232.6/232.6 kB 2.9 MB/s eta 0:00:00\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: pinecone-client in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.1.1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (2024.6.2)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (2.2.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\ricar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYFNyycCQ2p-",
        "outputId": "a3135211-c284-45a0-cce7-5ca742690b4d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import PyPDF2\n",
        "import random\n",
        "\n",
        "from pinecone import Pinecone\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwaQS_nXSQxf"
      },
      "source": [
        "# 3. Working with PDF files\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/1*FWwgOvUE660a04zoQplS7A.png)\n",
        "\n",
        "Source: https://betterprogramming.pub/building-a-multi-document-reader-and-chatbot-with-langchain-and-chatgpt-d1864d47e339\n",
        "\n",
        "\n",
        "### 3.1 Setting up API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "btc16h6ySPFA"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "os.getenv('OPENAI_API_KEY')\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwNqZjc2th6f"
      },
      "source": [
        "### 3.2 Loading a PDF file\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hV1ASHAHvm-G"
      },
      "outputs": [],
      "source": [
        "# Function to load a random PDF from a given directory\n",
        "def load_pdf(file_name):\n",
        "  pdf_file = open(file_name, 'rb')\n",
        "  pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "  text_from_pdf = \"\"\n",
        "\n",
        "  for page in range(len(pdf_reader.pages)):\n",
        "      text_from_pdf += pdf_reader.pages[page].extract_text()\n",
        "\n",
        "  return text_from_pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CkrkacuNwdHT"
      },
      "outputs": [],
      "source": [
        "# Function to chunk text by number of words or characters with a given size and overlap\n",
        "def chunk_text(text, chunk_size=1500, chunk_overlap=100, by='word'):\n",
        "    if by not in ['word', 'char']:\n",
        "        raise ValueError(\"Invalid value for 'by'. Use 'word' or 'char'.\")\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    if by == 'word':\n",
        "        text = text.split()\n",
        "    elif by == 'char':\n",
        "        text = text\n",
        "\n",
        "    current_chunk_start = 0\n",
        "    while current_chunk_start < len(text):\n",
        "        current_chunk_end = current_chunk_start + chunk_size\n",
        "\n",
        "        if by == 'word':\n",
        "            chunk = \" \".joint(text[current_chunk_start:current_chunk_end])\n",
        "        elif by == 'char':\n",
        "            chunk = text[current_chunk_start:current_chunk_end]\n",
        "\n",
        "        chunks.append(chunk)\n",
        "        current_chunk_start += chunk_size - chunk_overlap\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NrdKL3OH97rB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'As organizations rapidly deploy generative AI tools, survey respondents \\nexpect significant effects '"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_loaded = load_pdf(\"files/state_of_ai_docs.pdf\")\n",
        "pdf_loaded[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LfvY_eQH97o_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['As organizations rapidly deploy generative AI tools, survey respondents \\nexpect significant effects on their industries and workforces.The state of AI in \\n2023: Generative AI’s \\nbreakout year\\nAugust 2023The state of AI in 2023: Generative AI’s breakout yearThe latest annual McKinsey Global Survey  on the current  \\nstate of AI confirms the explosive growth of generative AI  \\n(gen AI) tools. Less than a year after many of these tools debuted, \\none-third of our survey respondents say their organizations are \\nusing gen AI regularly in at least one business function. Amid \\nrecent advances, AI has risen from a topic relegated to tech \\nemployees to a focus of company leaders: nearly one-quarter  \\nof surveyed C-suite executives say they are personally using  \\ngen AI tools for work, and more than one-quarter of respondents \\nfrom companies using AI say gen AI is already on their boards’ \\nagendas. What’s more, 40 percent of respondents say their \\norganizations will increase their investment in AI overall because \\nof advances in gen AI. The findings show that these are still early \\ndays for managing gen AI–related risks, with less than half of \\nrespondents saying their organizations are mitigating even the \\nrisk they consider most relevant: inaccuracy.\\nThe organizations that have already embedded AI capabilities \\nhave been the first to explore gen AI’s potential, and those seeing \\nthe most value from more traditional AI capabilities—a group we \\ncall AI high performers—are already outpaci',\n",
              " ' value from more traditional AI capabilities—a group we \\ncall AI high performers—are already outpacing others in their \\nadoption of gen AI tools.1\\nThe expected business disruption from gen AI is significant, and \\nrespondents predict meaningful changes to their workforces. They \\nanticipate workforce cuts in certain areas and large reskilling \\nefforts to address shifting talent needs. Yet while the use of gen AI \\nmight spur the adoption of other AI tools, we see few meaningful \\nincreases in organizations’ adoption of these technologies. The \\npercent of organizations adopting any AI tools has held steady since \\n2022, and adoption remains concentrated within a small number of \\nbusiness functions.\\n1 We define AI high performers as organizations that, according to respondents, attribute at least 20 percent of their EBIT  \\nto AI adoption. \\n1\\nThe state of AI in 2023: Generative AI’s breakout yearThe findings from the survey—which was in the field in mid-April 2023—show that, despite  \\ngen AI’s nascent public availability, experimentation with the tools is already relatively common, \\nand respondents expect the new capabilities to transform their industries. Gen AI has captured \\ninterest across the business population: individuals across regions, industries, and seniority \\nlevels are using gen AI for work and outside of work. Seventy-nine percent of all respondents say \\nthey’ve had at least some exposure to gen AI, either for work or outside of work, and 22\\xa0percent \\nsay they are regula']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks = chunk_text(pdf_loaded, by='char')\n",
        "chunks[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH4nGGFQyvnv"
      },
      "source": [
        "## 4. Building RAG system (Retrieval Augmented System)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "kJ73xZV0yvX0"
      },
      "outputs": [],
      "source": [
        "# Pinecone init\n",
        "pc = Pinecone(api_key=os.getenv('PINECODE_API_KEY'))\n",
        "\n",
        "# getting the index by name in pinecode\n",
        "index = pc.Index(\"rag-test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "0xpJHnj8y57J"
      },
      "outputs": [],
      "source": [
        "for i in range(len(chunks)):\n",
        "    vector = client.embeddings.create(\n",
        "        model = \"text-embedding-ada-002\",\n",
        "        input = chunks[i]\n",
        "    )\n",
        "\n",
        "    insert_stats = index.upsert(\n",
        "        vectors = [\n",
        "            (\n",
        "                str(i),\n",
        "                vector.data[0].embedding,\n",
        "                {\n",
        "                    \"org_text\": chunks[i]\n",
        "                }\n",
        "            )\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdGX-w1CVxGx"
      },
      "source": [
        "### 5. Building an interface to get proper answer based on the documentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SFhM14-NVUqJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As organizations rapidly deploy generative AI tools, survey respondents \n",
            "expect significant effects on their industries and workforces.The state of AI in \n",
            "2023: Generative AI’s \n",
            "breakout year\n",
            "August 2023The state of AI in 2023: Generative AI’s breakout yearThe latest annual McKinsey Global Survey  on the current  \n",
            "state of AI confirms the explosive growth of generative AI  \n",
            "(gen AI) tools. Less than a year after many of these tools debuted, \n",
            "one-third of our survey respondents say their organizations are \n",
            "using gen AI regularly in at least one business function. Amid \n",
            "recent advances, AI has risen from a topic relegated to tech \n",
            "employees to a focus of company leaders: nearly one-quarter  \n",
            "of surveyed C-suite executives say they are personally using  \n",
            "gen AI tools for work, and more than one-quarter of respondents \n",
            "from companies using AI say gen AI is already on their boards’ \n",
            "agendas. What’s more, 40 percent of respondents say their \n",
            "organizations will increase their investment in AI overall because \n",
            "of advances in gen AI. The findings show that these are still early \n",
            "days for managing gen AI–related risks, with less than half of \n",
            "respondents saying their organizations are mitigating even the \n",
            "risk they consider most relevant: inaccuracy.\n",
            "The organizations that have already embedded AI capabilities \n",
            "have been the first to explore gen AI’s potential, and those seeing \n",
            "the most value from more traditional AI capabilities—a group we \n",
            "call AI high performers—are already outpaci\n"
          ]
        }
      ],
      "source": [
        "user_input = \"The top AI trends in 2023/2024\"\n",
        "\n",
        "user_vector = client.embeddings.create(\n",
        "    model = \"text-embedding-ada-002\",\n",
        "    input = user_input\n",
        ")\n",
        "\n",
        "user_vector = user_vector.data[0].embedding\n",
        "\n",
        "matches = index.query(\n",
        "    vector = user_vector,\n",
        "    top_k = 1,\n",
        "    include_metadata = True\n",
        ")\n",
        "\n",
        "print(matches['matches'][0]['metadata']['org_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "q17g6FA9CLsa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but I can only provide information based on the text you provided about the state of AI in 2023. If you have any questions related to that, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"system\", \"content\": \"\"\"I want you to act as a support agent. Your name is \"My Super Assistant\". You will provide me with answers from the given info. If the answer is not included, say exactly \"Ooops! I don't know that.\" and stop after that. Refuse to answer any question not about the info. Never break character.\"\"\"}]\n",
        "messages.append({\"role\": \"user\", \"content\": matches['matches'][0]['metadata']['org_text']})\n",
        "messages.append({\"role\": \"user\", \"content\": \"trends in 20203 and\"})\n",
        "\n",
        "chat_messages = client.chat.completions.create(\n",
        "    model = \"gpt-3.5-turbo\",\n",
        "    messages = messages,\n",
        "    temperature=0,\n",
        "    max_tokens=400\n",
        ")\n",
        "\n",
        "print(chat_messages.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
