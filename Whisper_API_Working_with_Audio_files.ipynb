{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RCarteri/openAi_api/blob/main/Whisper_API_Working_with_Audio_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 6: Automatic Voice Message Translation System\n",
        "\n",
        "In this innovative project, we're going to build a system that bridges language barriers: an automatic voice message translation system. Utilizing OpenAI's Whisper API for state-of-the-art speech-to-text capabilities and the ChatCompletion API for accurate text translation, we will create an end-to-end solution that can translate any voice message into a chosen language.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "- **Whisper API for Speech Recognition**: Master the use of OpenAI's Whisper API to convert speech from voice messages into text.\n",
        "- **ChatCompletion API for Translation**: Learn how to implement the ChatCompletion API to translate the transcribed text into the desired language.\n",
        "- **Audio File Handling**: Develop skills for processing audio files in various formats within the Google Colab environment.\n",
        "\n",
        "## Preparation Checklist\n",
        "\n",
        "Before we start, make sure you have the following:\n",
        "\n",
        "- An active Google Colab account.\n",
        "- Basic to intermediate knowledge of Python programming.\n",
        "- Familiarity with handling APIs and audio data.\n",
        "- Access to OpenAI API keys with permissions to use Whisper and ChatCompletion APIs ([OpenAI](https://platform.openai.com/account/api-keys)).\n",
        "\n",
        "## Time to Break Down Language Barriers\n",
        "\n",
        "Get ready to dive into the world of automatic translation. By the end of this project, you will be capable of turning voice messages from any language into text and then into another language of your choice, all within moments!\n",
        "\n"
      ],
      "metadata": {
        "id": "vUehOJlORE-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Libraries import"
      ],
      "metadata": {
        "id": "NDlbXftjSHdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTBw517ySLF2",
        "outputId": "3620bba7-533a-4888-803e-e8609a6c6fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.1.1-py3-none-any.whl (217 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.1 httpx-0.25.1 openai-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYFNyycCQ2p-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Sending a first request to OpenAI API\n",
        "\n",
        "\n",
        "### 3.1 Setting up API Key"
      ],
      "metadata": {
        "id": "TwaQS_nXSQxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXXXXXXXXX\"\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "btc16h6ySPFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Processing Audio files with Whisper"
      ],
      "metadata": {
        "id": "9_OjVDWsUe4n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MsoHQlVsSywU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bIrZ8nmPX4u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio transcription"
      ],
      "metadata": {
        "id": "_Clc8Jk0TTt3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9wFnlLoNZAJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6euuSZWeRavf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translating to any language using ChatGPT and Whisper\n",
        "\n"
      ],
      "metadata": {
        "id": "mzr2K30GUbUN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtAclzsPT3Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3AglPCLR1_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kIb1xtNFSUAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VVAT-yCsPlDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}